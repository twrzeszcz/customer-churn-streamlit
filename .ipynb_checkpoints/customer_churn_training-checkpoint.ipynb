{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read our preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the correct metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is highly skewed as can be seen below. We have roughly **89%** of non-churners and **11%** of churners. For such classification problem accuracy is not a good metric because just a simple random guess will have a 89% chance to be a non-churner. Therefor we have to look at the different metrics here. A good choice can be a **recall** value for the churn class as it will tell us about the ratio of positive (churn) instances that are correctly detected by the classifier. That is the main goal cause if we predict non-churners as churners it won't be a big problem. This will only mean that the company will also take care of some customers who are actually not going to churn. The most important point is to identify as many churners as possible. There is obviously a trade-off between the recall and **precision** (accuracy of positive predictions) but here we should focus at the relatively high recall.\n",
    "\n",
    "The other related metrics to compare our models can be a **ROC curve** which plots a true positive rate (recall) versus a false positive rate (ratio of negative instances incorrectly classified as positive). This is exactly what are we looking for - the trade off between correctly classified churners and incorrectly classified non-churners. As a score with ROC curve we use a AUC score which is the area under curve. For the perfect classifier it is equal to 1 and for purely random to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='churn_flag')\n",
    "round(df['churn_flag'].value_counts() * 100 / len(df), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on the non-sampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try a bunch of simple non-tuned models to see what we can achieve with the current data. \n",
    "We have to split our data for features and targets and also standardize our features as they have very different range of values and it will hurt performance if we leave them as they are. Then we also split our data for train and test parts with ratio of 80/20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also worth to notice that we should not use **fit_transform** on both train and test sets and not scale our data before splitting. Fitting of the transformer should be done only on the training data so that during training there is no information taken from the test data. Otherwise standardization, so calculation of mean values and standard deviations is done also on the test set and it affects the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['churn_flag', 'Unnamed: 0'], axis=1)\n",
    "y= df['churn_flag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=120)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = {'lr': LogisticRegression(),\n",
    "      'rf': RandomForestClassifier(n_jobs=-1),\n",
    "      'svm': SVC(),\n",
    "      'xgb': XGBClassifier(n_jobs=-1),\n",
    "      'knn': KNeighborsClassifier(n_jobs=-1),\n",
    "      'nb': GaussianNB()}\n",
    "\n",
    "fit_model_base = {}\n",
    "\n",
    "for name, algo in clf.items():\n",
    "    model = algo.fit(X_train, y_train)\n",
    "    fit_model_base[name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained our models we can now display the classification report, plot the confusion matrix and the ROC curve for all of them to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in fit_model_base.items():\n",
    "    print(name)\n",
    "    print(classification_report(y_test, model.predict(X_test)))\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "    plt.show()\n",
    "    plot_roc_curve(model, X_test, y_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for name, model in fit_model_base.items():\n",
    "    plot_roc_curve(model, X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in fit_model_base.items():\n",
    "    pickle.dump(model, open(name + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see above, most of ours models have the same problem of detecing less than half of the churners correctly which is really bad. Only XGB classifier predicted more than a half correctly but still it is not what we are looking for. It looks like all of the models are focused on predicting correctly the non-churners. This is mainly because we have a highly imbalanced classes. There are certain ways of reducing this problem and we can look at the in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple of methods for resampling our data. One is to udersample the majority class to the size of the minority class but this reduces the size of the data significantly. The other method is to oversample the minority class to the size of the majority class by randomly replicating values. It is usually a source of overfitting. Much better method is to use a technique called **SMOTE** - Synthetic Minority Oversampling Technique. Here the subset of the data is taken from the minority class and new synthetic instances are created. Then they are added to the original dataset. We gonna use the modified **SMOTEENN** method which combines both oversampling and undersampling. The produced dataset is not equally balanced as we will see but much better distributed than the raw version and we will have more positive than negative instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['churn_flag', 'Unnamed: 0'], axis=1)\n",
    "y= df['churn_flag']\n",
    "\n",
    "from imblearn.combine import SMOTEENN \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=120)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "sm = SMOTEENN(random_state=120, n_jobs=-1)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train_res_sm, y_train_res_sm = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(y_train_res_sm.value_counts() * 100 / len(y_train_res_sm), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = {'lr_sm': LogisticRegression(),\n",
    "      'rf_sm': RandomForestClassifier(n_jobs=-1),\n",
    "      'svm_sm': SVC(),\n",
    "      'xgb_sm': XGBClassifier(n_jobs=-1),\n",
    "      'knn_sm': KNeighborsClassifier(n_jobs=-1),\n",
    "      'nb_sm': GaussianNB()}\n",
    "\n",
    "fit_model_sm = {}\n",
    "\n",
    "for name, algo in clf.items():\n",
    "    model = algo.fit(X_train_res_sm, y_train_res_sm)\n",
    "    fit_model_sm[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in fit_model_sm.items():\n",
    "    print(name)\n",
    "    print(classification_report(y_test, model.predict(X_test)))\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "    plt.show()\n",
    "    plot_roc_curve(model, X_test, y_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for name, model in fit_model_sm.items():\n",
    "    plot_roc_curve(model, X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in fit_model_sm.items():\n",
    "    pickle.dump(model, open(name + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see above, now we get much better predictions for churn class. More than 80% of recall is a really good score. Obviosuly we suffer also from the low precision and classify some part of the non-churners as churners but as mentioned before, we should focus mostly on the correct predictions for churners. Random Forest and XGB classifiers are doing the best from the tested models so we can tune them and see if better hyperparamters will improve their performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a big dataset hyperparameter tuning is a very lengthy process and require large computational effort. Therefore I have only tested Random Forest with hyperopt library which can automatically search in a very large space of parameters for the best ones for a specific model. I did this using Colab and saved the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned = pickle.load(open('models/rf_tuned.pkl', 'rb'))\n",
    "rf_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rf_tuned.predict(X_test)))\n",
    "plot_confusion_matrix(rf_tuned, X_test, y_test)\n",
    "plot_roc_curve(rf_tuned, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we don't get better results. I have only searched for 20 iterations so it was probably not enough.\n",
    "We can spend more time on hyperparameters tuning but our model is already really good so lets keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB and Random Forest provide also scores for every feature to measure how important they are for training. Let's look at the top 30 features predicted by both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_xgb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': fit_model_sm['xgb_sm'].feature_importances_\n",
    "})\n",
    "importances_xgb = importances_xgb.sort_values(by='Importance', ascending=False)\n",
    "importances_xgb = importances_xgb.set_index('Feature')\n",
    "\n",
    "importances_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': fit_model_sm['rf_sm'].feature_importances_\n",
    "})\n",
    "importances_rf = importances_rf.sort_values(by='Importance', ascending=False)\n",
    "importances_rf = importances_rf.set_index('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_rf[:30].plot(kind='bar', figsize=(20,3), title='Feature Importances Random Forest')\n",
    "plt.show()\n",
    "importances_xgb[:30].plot(kind='bar', figsize=(20,3), title='Feature Importances XGB Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 30 features predicted by both models are the same but their relative importance is different as we could see above. These features should be then monitored by the company as they are good indicators of whether the customer will churn or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further tuning we can choose either XGB or Random Forest as the final model cause they give very similar results in terms of AUC score and recall value for churn class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
